[training]
# Khoa: maxlen for discriminator input is always 50 because of convolution.

# Main .py file which will be used for the model
model_type: attention_GAN
model_discriminator_type: cnn_discriminator

# -----------------------------------------------------------------------------
# Pretrained model of Generator: Directory of generator.npz
# init: None
init: ~/Documents/Stage/GAN_NMT_model/data/attention-e512-r1024-adadelta_1e+00-bs80-bleu-each5000_do_d0.0-gc1-init_xavier-s1234.1-val001-bleu_38.850.npz

# Pretrained model of Discriminator: Directory of discriminator.npz
# initdis: None
initdis: ~/Documents/Stage/GAN_NMT_model/data/cnn_discriminator-e512-adadelta_1e+00-bs80-bleu-each50-gc1-init_xavier-s1234.1-val014-loss_0.165.npz

# -----------------------------------------------------------------------------
# Loop of Generator
generator_loop_num: 1

# Loop of Discriminator_reward
# Likelihood training if discriminator_reward_loop_num = 0
discriminator_reward_loop_num: 1

# Loop of Professor-Forcing
professor_forcing_loop_num: 1

# Loop of Discriminator
discriminator_loop_num: 1


# Maximum accuracy of Discriminator 
# (Stopping training condition of Discriminator)
max_acc: 0.95
min_acc: 0.75

# Monte Carlo search (monte_carlo_search = True)
# Getting directly from Discriminator (monte_carlo_search = False)
monte_carlo_search: False
maxlen: 10
rollnum: 5

# Professor-forcing
professor_forcing_reward: 1.0

# -----------------------------------------------------------------------------
# How much validation period will we wait
# to do early stopping
patience: 100

# Maximum number of epochs before stopping training
max_epochs: 1000000

# Validation start in terms of epochs
# validation frequency in terms of minibatch updates
valid_start: 1
valid_freq: 100

# 0: no, otherwise weight decay factor
decay_c: 0

# -1: no, otherwise maximum gradient norm
clip_c: 1.0
seed: 1234

# -----------------------------------------------------------------------------
[model]

# Optimizer: Professor-forcing and discriminator-reward have the different optimizers.
# adadelta, adam, sgd or rmsprop
optimizer_discriminator_reward: adadelta
optimizer_professor_forcing: adadelta

# Learning rate initial
lrate_discriminator_reward: 0.01
lrate_professor_forcing: 0.1

# batch size
batch_size: 2

# Using the same embedding for output and previous
tied_trg_emb: True
layer_norm: False

# Sort batches by target length or not
shuffle_mode: None

# 0: no, otherwise dropout probability
dropout: 0.0

# Embedding vector dimension
embedding_dim: 512 # For Generator
in_emb_dim: 512 # For language model
out_emb_dim: 512 # For language model

# RNN's hidden layer dimension
rnn_dim: 1024
enc_type: gru
dec_type: gru_cond

# Number of jobs while translating
njobs: 15

#Normalization of the cost
norm_cost: False

# Use BLEU as additional validation metric
valid_metric: bleu

weight_init: xavier

# 0: use all vocabulary, otherwise upper limit as integer
n_words_src: 30000
n_words_trg: 30000
n_words: 30000

# Where to save model params, weights and training log file
save_path:  ~/Documents/Stage/GAN_NMT_model/models

[model.dicts]
src: ~/Documents/Stage/GAN_NMT_model/data/train.en.vocab.pkl
trg: ~/Documents/Stage/GAN_NMT_model/data/train.fr.vocab.pkl
src_lm: ~/Documents/Stage/GAN_NMT_model/data/train.fr.vocab.pkl

[model.data]
train_src: ~/Documents/Stage/GAN_NMT_model/data/train.en
train_trg: ~/Documents/Stage/GAN_NMT_model/data/train.fr
valid_src: ~/Documents/Stage/GAN_NMT_model/data/valid.en
valid_trg: ~/Documents/Stage/GAN_NMT_model/data/valid.fr
